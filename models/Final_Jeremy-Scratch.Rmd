---
title: "DS7333 Final Case Study"
author: "Christian Nava, Jeremy Otsap, Martin Garcia, Michael Catalano, "
date: "8/1/2020"
output: 
    html_document:
        keep_md: true
---


```{r,echo=FALSE}

library(tidyverse) # data wrangling
library(modelr) # factor manipulation
library(skimr) # data exploration
library(na.tools) # for mean imputation
library(ggplot2) #visualization
library(reshape2) #wide / long for visualization
library(corrplot) # visualisation
library(VIM) # missing values
#library(vip)
#library(car) # modeling
library(glmnet) # logistic regression
library(caret) # model training & hyperparameter tuning
library(ROCR) # model validation
library(MASS) # model validation
library(ranger) # modeling Random Forest
library(e1071) # modeling Random Forest tuning
library(neuralnet) # forward feed neural net

```


Data set has 160.000 rows and  51 columns

```{r}
# loading data file from Azure Blob Storage as it exceeds the size threshold for github

final.df <- read.csv("https://modisdatawu2sqlml01.blob.core.windows.net/smu-data/ds7333/final_project.csv", header = T)

```


```{r}
#examine data

str(final.df)
summary(final.df)

```


## Data Munging

With the exception of the columns below, all of the 'X' predictors are numeric values:

* x24: factor [countries]
* x29: factor [month]
* x30: factor [day]
* **x32: factor [percentage change]**
* **x37: factor [dollars]**

R automatically can convert factors into 1-hot encoded values [depending on the package or algorithm], however columns x32 and x37 are being mislabeled as factor due to the $ and % characters. In order to correctly analye these columns we need to convert these to numeric values


```{r}

# convert percentage factor to numeric
as.numeric(gsub("\\%","",final.df$x32)) -> final.df$x32
# convert dollar factor to numeric
as.numeric(gsub("\\$","",final.df$x37)) -> final.df$x37

# validate x32 and x37 are now numeric
str(final.df[,c(33,38)])


```



## Missing Data: Imputation Method


Looking at the missing data, we can see that each column has its own unique set of missing values, which dont align to other columns / parameters. What that means is if we choose to select listwise deletion, it will delete an entire row for that column's one missing value. This will result in 1466 deleted rows



```{r}

sapply(final.df, function(x) sum(is.na(x))) 

# visualize missing data
aggr(final.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```


### Listwise Deletion

Far less than 1 percent of observations are removed when apply a listwise deletion approach. This certainly qualifies for listwise deletion and is the path of least resistance; however, the way the missing values are spread through out the data this **maximizes data loss**


```{r}

# output number of missing rows
print("total number of deleted rows: ")
print(dim(final.df)[1] - dim(na.omit(final.df))[1] )

# apply listwise deletion to new dataframe
final_impute.df <- na.omit(final.df)

print( "The percentage difference for listwise deletion is: "  )
print( (nrow(final.df)-nrow(final_impute.df))/nrow(final.df) )



```



### Mean Substitution + Listwise Deletion

For the final soluion we will use a hybrid approach:

**STEP 1: Listwise Deletion on Categorical Columns**

We validate there are **no missing values** on the following categorical columns, thus imputation method not required

* x24: factor [countries]
* x29: factor [month]
* x30: factor [day]



```{r}

# output number of remaining rows after applying listwise deletion
print("total number of missing rows in X24: ")
sum(is.na(final.df$x24))

print("total number of missing rows in X29: ")
sum(is.na(final.df$x29))

print("total number of missing rows in X30: ")
sum(is.na(final.df$x30))


```



**STEP 2: Mean Substitution on Remaining Numerical Columns**

For all numerical columns we will use mean substitution


```{r}

# create new dataframe
final.df -> final_msub.df

# NOTE: do not globally sapply na.mean to entire data frame or it will ERRONEOUSLY convert categorical columns to numeric

#sapply(final_msub.df, function(x) na.mean(x) ) %>% as.data.frame() -> final_msub.df

#sapply(final.df, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x) %>% as.data.frame() -> final_msub.df


# NOTE: apply(final.df, 2, is.na) forces to categorical
#na.mean(final_msub.df$x0) -> final_msub.df$x0

# loop through na.mean
loop_length <- length(final_msub.df)

for (i in c(1:24,26:29,32:51) ) {
  na.mean(final_msub.df[,i]) -> final_msub.df[,i]
  i = i + 1
}

```


Verify new imputed dataframe

```{r}

# verify x24, x29, and x30 are still categorical
str(final_msub.df[,c(25, 30, 31)])

# verify all missing values have been imputd
sapply(final_msub.df, function(x) sum(is.na(x))) 

# visualize missing data
aggr(final_msub.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)


```




##Data Visualization

We can see x37 [likely the dollars] has a dramatically larger range than the rest of the attributes, thus scaling and centering will probably be required. Or optionally we can divide by 1000 to represent $1000's of dollars


```{r, echo=FALSE}

ggplot(data = melt(final_msub.df), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable)) + coord_flip()
```


```{r}

# compare range of x37 raw and divided by 1000

summary(final_msub.df$x37)
summary((final_msub.df$x37)/1000)

```






## Model Baseline: Logistic Regression


Note: For the caret package to properly process binomial classification models, the response must be categorical binary


```{r}
# CATEGORIZE Y 
final_msub.df <- final_msub.df %>% mutate(y = factor(y) )

```



**Train / Test Sample Split**

One of the challenges of creating the test / train split is accounting for the unbalanced level of your response variable. In this case:


0 - 95803
1 - 64197 
 
Thus we need to insure both the train and test samples reflect this ratio


```{r}

# creating the 80 data partition 
final_split <- createDataPartition(final_msub.df$y, p = 0.8, list = F)
# including 80 for training set
final_train.df <- final_msub.df[final_split,] 
# excluding 80 for testing set
final_test.df <- final_msub.df[-final_split,]
# validating
dim(final_test.df)
dim(final_train.df)

```


```{r}

# compare level split of main data frame
table(final_msub.df$y)

# validating sample maintained ratio for response levels
table(final_test.df$y)
table(final_train.df$y)

```


**CONCERN 1: LARGE DATA SET**

We can use random selection vs the default grid searching. The scope of this is beyond this exercise, but essentially with a large number of hyperparameters, random may be a better option. More info can be read on the Caret package here:

https://topepo.github.io/caret/random-hyperparameter-search.html


Additionally we allow parallel processing to optimize the speed of the modeling


**CONCERN 2: WIDE RANGE OF VALUES ACROSS PREDICTORS**

As per above the ranges vary significantly across the predictors, esp x37, thus we will scale and center the values

Note: given the huge data size, we can use x37/1000 instead of scale/center if procesing time becomes an issue



###Baseline Model

No preprocessing [i.e. scaling or centering]
No hyperparameter tuning
No feature selection [i.e. stepwise]

Single-threaded CPU: 9 seconds


```{r}
# create logistic regression model on train data
# R will automatically create dummy variables on factors
final_train.logit <- glm(y ~ ., data = final_train.df, family = binomial("logit") )
summary(final_train.logit)


```




**Visualize the coefficients of the Full Logistic Model**

Visualizing the coefficients, we can see the most *influential* factors are the type of Internet Service and the type of contract, which were included in significant factors above

```{r}
# visualize coefficients
as.data.frame(final_train.logit$coefficients) %>% ggplot(aes(y = .[,1], x = rownames(.)) ) + geom_col() + theme( axis.text = element_text(angle = 90, size = rel(0.7)) )
# standard graphics alternative
# barplot( final_train.logit$coefficients, names.arg = F, col = rainbow(31), legend.text = names(final_train.logit$coefficient) )
```



**ROC Curve for Full Logistic Model**

So now lets validate the accuracy of our model on the test data set. First lets take a look at a ROC curve for the model. The AUC is 75.9 


```{r}
# create predictions
predict(
  final_train.logit, 
  newdata = final_test.df,
  type = "response"
  ) -> final_logit.pred

# ROC curve
performance(
  ROCR::prediction(final_logit.pred, final_test.df$y),
  measure = "tpr",
  x.measure = "fpr"
) -> final_logit.perf 

plot(final_logit.perf)

# AUC value
print("AUC Value for this model is ")
performance(
  ROCR::prediction(final_logit.pred, final_test.df$y),
  measure = "auc"
)@y.values[[1]]
```




**Confusion Matrix**

Note: to assure an accurate confusion matrix we explicity specify 1 as the reference, as opposed to letting the software randomly decide.

NOTE: first we must coerce the probabilities to either be 1 or 0 based on a 50% threshold

```{r}
# split into "1" and "0" factors based on 0.5 thresdshold
as.numeric(final_logit.pred > 0.5 ) %>% as.factor() -> final_logit_factor.pred

```


Confusion Matrix and Statistics

          Reference
Prediction     1     0
         1  6738  3372
         0  6101 15788

Accuracy : 0.704 
Sensitivity : 0.5248 
Specificity : 0.8240


False Positive costs = $10 * 6101
False Negative costs = $500 * 3372

**TOTAL COST: $1,747,010**




```{r}
# confusion matrix
caret::confusionMatrix(
  data = relevel(final_logit_factor.pred, ref = "1"),
  reference = relevel(final_test.df$y , ref = "1")
)
```










## Model: Random Forest


**RANGER ALGORITHM**

We are using the Ranger package to fit the Random Forest as opposed to the default rpart algorithms or randomForest package due to it's significantly faster C++ implementation that allows for multi-threaded processing

TOTAL PROCESSING TIME: 5 minutes


```{r}

# default mtry for classification is sqrt(# of features)
mtry_final <- floor(sqrt(ncol(final_train.df)))
#tuneGrid = expand.grid(mtry = c(4,5,6))

# training RF model

ranger(
  y ~ .,
  data = final_train.df,
  mtry = mtry_final,
  respect.unordered.factors = "order",
  importance = "permutation", # "impurity",
  verbose = T,
  seed = 665
) -> final_train.rf

```


[1] "RMSE for model is: "
[1] 0.2702997


Confusion matrix for **TRAINING** data just to give an idea. However we officially use only the **TEST** data to determine cost


    predicted
true     0     1
   0 73377  3266
   1  6086 45272



**Model Hyperparameters**

The following are the model default hyperparameters: 

* **mtry** How many random variables should be included in each tree?
* **min.node.size** What is minimum depth of each tree before a split can occur?
* **splitrule** What is the criteria to split trees? [i.e. to minimize node impurity]


**Ranger Defaults**
* mtry for Classification: sqrt(# of predictors) ~7 in this case
* mtry for Regression: (# of predictors) / 3
* min.node.size: 1 for classification or 5 for regression
* splitrule: "gini" for classification or SSE for regression



[1] "Final model's mtry: "
[1] 7
[1] "Final model's min.node.size: "
[1] 1
[1] "Final model's num.trees: "
[1] 500
[1] "Final model's splitrule: "
[1] "gini"


```{r}

#RMSE
print("RMSE for model is: ")
print( sqrt(final_train.rf$prediction.error) )

# confusion matrix on TRAINING data
final_train.rf$confusion.matrix



# best tuning parameters

print("Final model's mtry: ")
print(final_train.rf$mtry)

print("Final model's min.node.size: ")
final_train.rf$min.node.size

print("Final model's num.trees: ")
final_train.rf$num.trees

print("Final model's splitrule: ")
final_train.rf$splitrule

print("Final model's variable importance mode: ")
final_train.rf$importance.mode


```


### Evaluating model with test predictions




```{r}
# predictions
predict(
  final_train.rf,
  data = final_test.df,
  ) -> final_forest.pred
```

**AUC and RMSE**

"AUC Value for this model is "
0.9194343
"RMSE Value for this model is "
0.2701315


```{r}
# AUC plot
# NOTE: must convert values to numeric to use function
performance(
  ROCR::prediction( as.numeric(final_forest.pred$predictions),
              as.numeric(final_test.df$y) ),
  measure = "tpr",
  x.measure = "fpr"
) -> final_forest.perf 
plot(final_forest.perf)

# AUC value
print("AUC Value for this model is ")
performance(
  ROCR::prediction( as.numeric(final_forest.pred$predictions),
              as.numeric(final_test.df$y) ),
  measure = "auc",
  )@y.values[[1]] 

# RMSE
print("RMSE Value for this model is ")
RMSE( as.numeric(final_forest.pred$predictions), 
      as.numeric(final_test.df$y) )
```


**Confusion Matrix**

Note: to assure an accurate confusion matrix we explicity specify 1 as the reference, as opposed to letting the software randomly decide.


Confusion Matrix and Statistics

          Reference
Prediction     1     0
         1 11335   833
         0  1504 18327

Accuracy : 0.927
Sensitivity : 0.8829
Specificity : 0.9565


False Positive costs = $10 * 1504
False Negative costs = $500 * 833

**TOTAL COST: $431,540**

We've brought our cost down ~25% of the baseline logistic regression model





```{r}
# confusion matrix
caret::confusionMatrix(
  data = relevel(final_forest.pred$predictions, ref = "1"),
  reference = relevel(final_test.df$y , ref = "1")
)
```



**Variable Importance & Impurity**

Given the output of our initial ranger random forest model, we can limit to variables found to have the highest importance:


x23
x49
x20
x48
x42
x37
x12
x40
x27
x6
x2
x28
x41
x38
x7
x46
x32


```{r}

# sexier importance plot
final_ranger_imp.df <- as.data.frame(final_train.rf$variable.importance)

#rownames
final_ranger_imp.df$x_label <- rownames(final_ranger_imp.df )

#rename column
names(final_ranger_imp.df)[1] <- "x_value"


#ggplot
ggplot(final_ranger_imp.df, aes(x=reorder(x_label,x_value), y=x_value,fill=x_value)) + geom_bar(stat = 'identity') + coord_flip() + ylab("Importance") + xlab("") + ggtitle("Variable Importance") + guides(fill=F) + scale_fill_gradient(low="gray", high="blue")




```




###TUNING RANDOM FOREST ALGORITHM

**NOTE: DO NOT RUN - 11 Hour Runtime **

I'll put my notes here and then will notate the CORRECT model to run which already has the optimized tuning grid

------

**Hyperparameter Tuning**

As per above, there are a number of hyperparameters that can be optimized to improve model performance. We use Caret as the training engine to optimize Ranger's hyperparameters as well as executing the 5 fold cross validation and normalizing the data 

We set the tuning grid as follows
* attempt **mtry** values of 5,6, and 7
* attempt **min.node.size** values of 1 and 2
* attempt all 3 different types of **splitrule** values: "gini", "extratrees", "hellinger" Additionally 
Given the higher cost of False Negatives we set the target **metric** to Specificity, rather than the default Accuracy. 

*Specificity* is essentially how well the model predicts True Negatives. I.e. the ability of the model to correctly identify those without cancer. In other words of the ACTUAL healthy patients, how many were accurately PREDICTED to NOT have cancer



We also set the importance from the default impurity to permutation [see note below on Gini vs Permutation]


**Improving Processing Time**

In addition to using the Ranger algorithm, having Caret run the model training allows us to use multiple CPU's in parallel via the allowParallel field.

Also will reduce Number of trees to 300 from default of 500

Using a Random Search over a Grid Search can help improve performance. In Grid Search, Caret sets up a grid of hyperparameter values and for each combination, trains a model and scores on the testing data. Conversely, using a random search sets up a grid of hyperparameter values and selects random combinations to train and score the model


All of these factors resulted in an **11 hour runtime**


------

**Splitrule: Hellinger **

Note: this is ONLY for **binary** classification with level imbalances, thus we do NOT use in our final tuning grid

When working with imbalanced data, the minority class has a considerably smaller representation in the data. We want to take this fact into consideration when choosing the best split of a decision tree model. The commonly used scorers for split decision are Gini and Entropy. Both favor splits which result in an uneven class distribution (e.g. 90–10) over splits which lead to an even distribution of classes (e.g. 50–50). This is a major problem, take for example an imbalanced data set with 90–10 class balance. A split which will produce 90–10 class balance in the children population does not improve the class separation in any way but gets high Gini and Entropy scores. Hellinger Distance based split, will give high score to a split separating the classes in the best way relative to the parent population. Thus, in a population with 80–20 class balance, a split producing 80–20 split will get low score 50–50 split will get a high score






**Permutation vs Gini**

Impurity importance can be calculated fast, however it has been shown to be biased in favor of continuous and high cardinality variables. Each time a break point is selected in a variable, every level of the variable is tested to find the best break point. Continuous or high cardinality variables will have many more split points, which results in the “multiple testing” problem. That is, there is a higher probability that by chance that variable happens to predict the outcome well, since variables where more splits are tried will appear more often in the tree.

Permutation importance is more reliable, albeit more computationally expensive than mean decrease in impurity. The basic idea is to consider a variable important if it has a positive effect on the prediction accuracy (classification), or MSE (regression). This metric is applicable to any model, not just random forests. The risk of using this metric is a potential bias towards collinear predictive variables.




Random forest standard splitting rule is biased when predictor variables vary in their scale. This also has a substantial impact on the variable importance. In the case of the Gini variable importance, predictor variables with many categories or numerical values receive on average a higher variable importance than binary variables if both variables have no influence on the outcome variable. The permutation variable importance remains unbiased in these cases, but there is a higher variance of the variable importance for variables with many categories

Higher mtry values lead to lower variable importance of weak regressors. The values of the variable importance from the standard random forest were far less dependent on mtry than the ones from the conditional inference forests. This was due to the much larger size (i.e., number of splits untilthe terminal node) of individual trees in the standard random forest. 

Decreasing the tree size (for example by setting a higher node size value) while setting mtry to a small value leads to more equal values of the variable importances of all variables, because there was less chance that relevant variables were chosen in the splitting procedure






```{r}

##################
### DO NOT RUN ###
##################
### 11 HOURS   ###
##################


# cluster preparation for parallel CPU
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # leave 1 core for OS
registerDoParallel(cluster)

# training grid
trainControl(
  method = "cv",
  number = 5,
  summaryFunction = twoClassSummary, #prSummary = c('AUC', 'Precision', 'Recall' )
  savePredictions = "final", 
  classProbs = F, 
  verboseIter = T,
  search = "random",
  allowParallel = T
  ) -> final_caret.grid

# tuning grid

final_tune.grid <- expand.grid( 
  mtry = c(5,7,9), 
  splitrule = c( "gini", "extratrees", "hellinger"), 
  min.node.size = c(1,2) )

# training RF model
train(
  y ~ .,
  data = final_train.df,
  method = "ranger",
  trControl = final_caret.grid, 
  num.threads = (detectCores() - 1),
  preProcess = c("scale","center"),
  tuneGrid = final_tune.grid,
  importance = 'permutation', #'impurity',
  metric = "Spec" # "Accuracy" or "Sens"
) -> final_train.caret


### SHUTDOWN CLUSTER
stopCluster(cluster)
#registerDoSEQ()

```







**Final Model**

45 minutes



Ranger result

Call:
 ranger::ranger(dependent.variable.name = ".outcome", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) 

Type:                             Classification 
Number of trees:                  500 
Sample size:                      128001 
Number of independent variables:  67 
Mtry:                             **5** 
Target node size:                 **2** 
Variable importance mode:         **permutation**
Splitrule:                        **extratrees** 
Number of random splits:          1 
OOB prediction error:             11.28 % 




### Evaluating model with test predictions

Make predictions w/ best model
* mtry: 5
* splitrule: extra trees
* min.node.size: 2

**Ranger result**

Call:
 ranger::ranger(dependent.variable.name = ".outcome", data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) 

Type:                             Classification 
Number of trees:                  300 
Sample size:                      128001 
Number of independent variables:  67 
Mtry:                             7 
Target node size:                 2 
Variable importance mode:         permutation 
Splitrule:                        extratrees 
Number of random splits:          1 
OOB prediction error:             9.94 % 


```{r}

########################
### USE THIS INSTEAD ###
########################




# cluster preparation for parallel CPU
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # leave 1 core for OS
registerDoParallel(cluster)

# training grid
trainControl(
  method = "cv",
  number = 5,
  summaryFunction = twoClassSummary, 
  savePredictions = "final", 
  classProbs = F, 
  verboseIter = T,
  search = "random",
  allowParallel = T
  ) -> final_caret.grid

# tuning grid

final_tune.grid <- expand.grid( 
  mtry = 7, 
  splitrule = "extratrees", 
  min.node.size = 2)

# training RF model
train(
  y ~ .,
  data = final_train.df,
  method = "ranger",
  trControl = final_caret.grid, 
  num.threads = (detectCores() - 1),
  preProcess = c("scale","center"),
  tuneGrid = final_tune.grid,
  num.trees = 300,
  importance = 'permutation', #'impurity',
  metric = "Spec" # "Accuracy" or "Sens"
) -> final_train.caret


### SHUTDOWN CLUSTER
stopCluster(cluster)
#registerDoSEQ()




```







```{r}
# best tuning parameter
final_train.caret$bestTune
#final model
final_train.caret$finalModel
```



```{r}
# predictions
predict(final_train.caret, final_test.df) -> final_caret.pred

```


**AUC and RMSE**




[1] "AUC Value for this model is "
[1] 0.8922274
[1] "RMSE Value for this model is "
[1] 0.3092883


```{r}
# AUC plot
# NOTE: must convert values to numeric to use function
performance(
  ROCR::prediction( as.numeric(final_caret.pred), 
              as.numeric(final_test.df$y)
              ),
  measure = "tpr",
  x.measure = "fpr"
) -> final_caret.perf 
plot(final_caret.perf)

# AUC value
print("AUC Value for this model is ")
performance(
  ROCR::prediction( as.numeric(final_caret.pred),
              as.numeric(final_test.df$y) ),
  measure = "auc",
  )@y.values[[1]] 

# RMSE
print("RMSE Value for this model is ")
RMSE( as.numeric(final_caret.pred), 
      as.numeric(final_test.df$y) )
```


**Confusion Matrix**

Note: to assure an accurate confusion matrix we explicity specify 1 as the reference, as opposed to letting the software randomly decide.

Overall our model **Accuracy** went down slightly, however our **Specificity** jumped up to 95.8%

Accuracy : 0.9043 
Sensitivity : 0.8309           
Specificity : 0.9535 


Confusion Matrix and Statistics

          Reference
Prediction     1     0
         1 10668   890
         0  2171 18270
                                          

False Positive costs = $10 * 2171
False Negative costs = $500 * 890

**TOTAL COST: $466,710**


        


```{r}
# confusion matrix
caret::confusionMatrix(
  data = relevel(final_caret.pred, ref = "1"),
  reference = relevel(final_test.df$y , ref = "1")
)
```




**Variable Importance & Impurity**


```{r}

# bare bones importance plot
# unfortunatley ranger objects do not work for varImpPlot
plot(varImp(final_train.caret), main = "Variable Importance")

```


```{r}

# sexier importance plot
final_caret_imp.df <- as.data.frame(final_train.caret$finalModel$variable.importance)

#rownames
final_caret_imp.df$x_label <- rownames(final_caret_imp.df)

#rename column
names(final_caret_imp.df)[1] <- "x_value"


#ggplot
ggplot(final_caret_imp.df, aes(x=reorder(x_label,x_value), y=x_value,fill=x_value)) + geom_bar(stat = 'identity') + coord_flip() + ylab("Importance") + xlab("") + ggtitle("Variable Importance") + guides(fill=F) + scale_fill_gradient(low="gray", high="blue")




```





##Neurel Net Using Caret Training




###Model: FOrward Feed Neural Net



**Runtime: 2 minutes**


```{r}

############################
### FULL MODEL: ALL VARIABLES
############################


# cluster preparation for parallel CPU
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # leave 1 core for OS
registerDoParallel(cluster)

# training grid
nn.grid <- expand.grid(size = c(8,8), decay = c(0.05))

trainControl(
  method = "cv", 
  number = 5,
  summaryFunction = twoClassSummary,
  allowParallel = T,
  classProbs = F, 
  verboseIter = T, 
  search = "random"
  ) -> full_nnet.grid



### NOTE: the standard formula syntax of "y ~ ." is not accepted in the neuralnet() function 
## Must first create formula object then pass it to neuralnet() function

train_names <- names(final_train.df)
train_formula_nn <- as.formula(paste("y ~", paste(train_names[!train_names %in% "y"], collapse = " + ")))

# training NNET model
train(train_formula,
  data = final_train.df,
  method = "nnet",
  metric = "Spec",  # "Accuracy",  # "ROC", 
  trControl = full_nnet.grid, 
  num.threads = (detectCores() - 1),
  preProcess = "scale",
  tuneGrid = nn.grid
) -> full_train.nnet


### SHUTDOWN CLUSTER
stopCluster(cluster)
#registerDoSEQ()



```




### Evaluating model with test predictions

```{r}
# predictions
predict(full_train.nnet, final_test.df) -> full_nnet.pred
```



**AUC and RMSE**

Not quite as good as untuned Random Forest
 
[1] "AUC Value for this model is "
[1] 0.8599844
[1] "RMSE Value for this model is "
[1] 0.3652111


```{r}
# AUC plot
# NOTE: must convert values to numeric to use function
performance(
  ROCR::prediction( 
    as.numeric(full_nnet.pred), 
    as.numeric(final_test.df$y) 
    ),
  measure = "tpr",
  x.measure = "fpr"
) -> full_nnet.perf 
plot(full_nnet.perf)

# AUC value
print("AUC Value for this model is ")
performance(
  ROCR::prediction( 
    as.numeric(full_nnet.pred), 
    as.numeric(final_test.df$y) 
    ),
  measure = "auc",
  )@y.values[[1]] 

# RMSE
print("RMSE Value for this model is ")
RMSE( as.numeric(full_nnet.pred), as.numeric(final_test.df$y) )
```




**Confusion Matrix**

Somewhat less performant than untuned Random Forest

          Reference
Prediction     1     0
         1 10610  2039
         0  2229 17121
                                          
Accuracy : 0.8666   
Sensitivity : 0.8264          
Specificity : 0.8936  


False Positive costs = $10 * 2229
False Negative costs = $500 * 2039

**TOTAL: $1,041,790**





```{r}
# confusion matrix
caret::confusionMatrix(
  data = relevel(full_nnet.pred, ref = "1"),
  reference = relevel(final_test.df$y, ref = "1")
)
```





###Model: Forward Feed Neural Net - Important Paremeters


Given the output of our random forest model, we will limit to variables found to have the highest importance, as per the earlier plot above:

x23
x49
x20
x48
x42
x37
x12
x40
x27
x6
x2
x28
x41
x38
x7
x46
x32

**Runtime: 2 minutes**


```{r}

############################
### IMPORTANT VARIABLES FROM RANGER
############################


# cluster preparation for parallel CPU
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # leave 1 core for OS
registerDoParallel(cluster)

# training grid
nn.grid <- expand.grid(size = c(7,7), decay = c(0.01))

trainControl(
  method = "cv", 
  number = 5,
  summaryFunction = twoClassSummary,
  allowParallel = T,
  classProbs = F, 
  verboseIter = T, 
  search = "random"
  ) -> final_nnet.grid

# formula
train_formula <- as.formula(y ~ x23 + x49 + x20 + x48 + x42 + x37 + x12 + x40 + x27 + x6 + x2 + x28 + x41 + x38 + x7 + x46 + x32)

# training NNET model
train(train_formula,
  data = final_train.df,
  method = "nnet",
  metric = "Spec",  # "ROC", 
  trControl = final_nnet.grid, 
  num.threads = (detectCores() - 1),
  preProcess = "scale",
  tuneGrid = nn.grid
) -> final_train.nnet


### SHUTDOWN CLUSTER
stopCluster(cluster)
#registerDoSEQ()

```





### Evaluating model with test predictions

```{r}
# predictions
predict(final_train.nnet, final_test.df) -> final_nnet.pred
```



**AUC and RMSE**

Not quite as good as untuned Random Forest
 
[1] "AUC Value for this model is "
[1] 0.8451208
[1] "RMSE Value for this model is "
[1] 0.3851196


```{r}
# AUC plot
# NOTE: must convert values to numeric to use function
performance(
  ROCR::prediction( 
    as.numeric(final_nnet.pred), 
    as.numeric(final_test.df$y) 
    ),
  measure = "tpr",
  x.measure = "fpr"
) -> final_nnet.perf 
plot(final_nnet.perf)

# AUC value
print("AUC Value for this model is ")
performance(
  ROCR::prediction( 
    as.numeric(final_nnet.pred), 
    as.numeric(final_test.df$y) 
    ),
  measure = "auc",
  )@y.values[[1]] 

# RMSE
print("RMSE Value for this model is ")
RMSE( as.numeric(final_nnet.pred), as.numeric(final_test.df$y) )
```




**Confusion Matrix**

Somewhat less performant than untuned Random Forest

Confusion Matrix and Statistics

          Reference
Prediction     1     0
         1 10109  1937
         0  2730 17223
                                         
Accuracy : 0.8542 
Sensitivity : 0.7874         
Specificity : 0.8989



False Positive costs = $10 * 2730
False Negative costs = $500 * 1937

**TOTAL: $995,800**


```{r}
# confusion matrix
caret::confusionMatrix(
  data = relevel(final_nnet.pred, ref = "1"),
  reference = relevel(final_test.df$y, ref = "1")
)
```





