---
title: "Cherry Blossom Race: Case Study"
author: "Martin Garcia, Michael Catalano, Jeremy Otsap, Christian Nava"
date: "5/30/2020"
output: html_document
---
  
```{r, include=FALSE}

### loading libraries
library(tidyverse)
library(XML)
library(stringi)
library(rvest)
library(RCurl)
library(xml2)
library(purrr)
library(ggplot2)
library(VIM)
library(SiZer)
library(changepoint.np)
```

## Introduction

#### The Cherry Blossom Ten Mile Run is a race held in Washington, D.C. each spring as the cherry blossom trees are in full bloom. The results of this race are published publicly on a website (http://www.cherryblossom.org). These race data provide a challenging candidate for web scraping and analysis due to its online format. A web scraper is faced with a host of issues when attempting to manipulate this data, including data discrepancies, input errors, formatting issues, missing headers, and inconsistent values across columns.

#### The bulk of this analysis is focused on gathering clean, tidy, and tabular data through web scraping and preprocessing. Next, we dive into the race results, conduct some statistical analysis to understand distributions in age and race times over the years 1999-2012, and finish with a change-point analysis. The goal of this change-point analysis is to determine at what ages the changes to average run time are occuring.

## Method

#### The primary tool used in this case study is R, with a host of packages to assist in the web scraping and follow-on analysis. Helper functions are created with the goal of making our lives easier and keeping the preprocessing dynamic, although this poses challenges, as many race years required a tailored approach to work through different formatting issues.

#### Insert Jeremy's portion here

#### For the statistical analysis portion of the case, our approach was to use subgroups of runners with roughly the same age as our "smoother" by averaging run performance across each group. Specifically, we group the runners into 10-year age intervals and plot the summaries for each subgroup in our summaries and graphs.

#### For the change-point analysis, we have conducted two tests, one using piecewise linear modeling for detecting a single change-point, and a non-parametric test (PELT) for detecting multiple change-points in ages by average run time. 

## Web Scraping & Data Cleaning

```{r,  results='hide'}
# women race results
urlWomen <- c(
  'http://cherryblossom.org/results/1999/cb99f.html',
  'http://cherryblossom.org/results/2000/Cb003f.htm',
  'http://cherryblossom.org/results/2001/oof_f.html',
  'http://cherryblossom.org/results/2002/ooff.htm',
  'http://cherryblossom.org/results/2003/CB03-F.HTM',
  'http://cherryblossom.org/results/2004/women.htm',
  'http://cherryblossom.org/results/2005/CB05-F.htm',
  'http://cherryblossom.org/results/2006/women.htm',
  'http://cherryblossom.org/results/2007/women.htm',
  'http://cherryblossom.org/results/2008/women.htm',
  'http://cherryblossom.org/results/2009/09cucb-F.htm',
  'http://cherryblossom.org/results/2010/2010cucb10m-f.htm',
  'http://cherryblossom.org/results/2011/2011cucb10m-f.htm',
  'http://cherryblossom.org/results/2012/2012cucb10m-f.htm'
)

```

```{r,  results='hide'}
# helper function that reads women URLs, then the table nodes marked with 'pre', and then splits on new lines.
extractTable <- function(url) {
  read_html(url) %>% 
  html_nodes('pre') %>% 
  html_text() %>% 
  str_split('\\r\\n') %>% 
  .[[1]]
}


# creates list with each row being a string of the year run.
womenTable <- map(urlWomen, extractTable)

# validate row counts for each year
# NOTE: first one is 1999, second is 2000, etc
# note that 1999 and 2000 have issues, much lower than expected row counts
map_int(womenTable, length)

#compare [[1]] vs [[6]] of womenTable
womenTable[[1]] # complete mess
womenTable[[6]] # even line breaks

# first entry of 1999
# note its not performing line breaks
str_sub(womenTable[[1]], 1, 210)


# validate we can force a split on new line breaks
str_split(womenTable[[1]], '\\n')[[1]]

# update womenTable
str_split(womenTable[[1]], '\\n')[[1]] -> womenTable[[1]]

```

```{r,  results='hide'}
# need to update extractTable() to process 1999 as a new line split
# also for 2000 data to parse <font> html tag vs <pre> html tag

extractTable <- function(url, year = 2001, female = TRUE) {
  selector <- if (year == 2000) 'font' else 'pre'
  regexp <- if (year == 1999) '\\n' else '\\r\\n'
  #read urls and respective table tags
  result <- read_html(url) %>% 
    html_nodes(selector)
  
  if (year == 2000) result <- result[[4]]
  #parse htmltext
  result <- result %>% html_text()
  #splits the table nodes with respective function for year
  if (year == 2009 && female == FALSE) return(result)
  
  result %>% str_split(regexp) %>% .[[1]]
}

years <- 1999:2012
womenTable <- map2(urlWomen, years, extractTable)

```

```{r}
# validate row counts by year
names(womenTable) <- years
map_int(womenTable, length)
```
#### Now we can visualize each year with its respective row counts. Next, we'll create individual txt files for each year. Warning - this will create a new folder housing these text files

```{r,  results='hide'}
# create dir
dir.create('women')

# create txt file for each year
walk2(womenTable, paste('women', paste(years, 'txt', sep = '.'), sep = '/'), writeLines)

### HELPER FUNCTIONS FOR COLUMNS ###

# find start / end of each column
findColLocs <- function(spacerRow) {
  
  spaceLocs = gregexpr(" ", spacerRow)[[1]]
  rowLength = nchar(spacerRow)
  
  # safeguard against more ==== lines
  if (substring(spacerRow, rowLength, rowLength) != " ")
    return( c(0, spaceLocs, rowLength + 1))
  else return(c(0, spaceLocs))
}

# extract columns 

selectCols <- function(shortColNames, headerRow, searchLocs) {
  sapply(shortColNames, function(shortName, headerRow, searchLocs){
    
    startPos = regexpr(shortName, headerRow)[[1]]
    
    if (startPos == -1) return( c(NA, NA) )
    
    index = sum(startPos >= searchLocs)
    c(searchLocs[index] + 1, searchLocs[index + 1])
  }, 
  
  headerRow = headerRow, searchLocs = searchLocs )
}

# extract only desired columns

extractVariables <- function(file, varNames =c("name", "home", "ag", "gun", "net", "time")) {
    # find the index of the row with === lines
    eqIndex = grep("^===", file)
    # extract key rows and the data
    spacerRow = file[eqIndex] 
    headerRow = tolower(file[ eqIndex - 1 ])
    body = file[ -(1 : eqIndex) ]
    
    # Obtain the starting and ending positions of variables
    searchLocs = findColLocs(spacerRow)
    locCols = selectCols(varNames, headerRow, searchLocs)
    Values = mapply(substr, list(body), start = locCols[1, ], 
                    stop = locCols[2, ])
    colnames(Values) = varNames
    
    invisible(Values)
  }
```

```{r,  results='hide'} 
wfilenames <- paste("women/", 1999:2012, ".txt", sep = "")
womenFiles <- lapply(wfilenames, readLines)
names(womenFiles) <- 1999:2012
```

#### Validate 2001 Header problem and correct

```{r}
# NOTE: 2001 HAS A PROBLEM
# NO HEADERS
womenFiles[['2001']][1:15]

# check 2002
womenFiles[['2002']][1:15]

# copy headers from 2002 into 2001
womenFiles[['2002']][2:3] -> womenFiles[['2001']][2:3]

# check 2001 again
womenFiles[['2001']][1:15]

# Retry w/ corrected 2001
womenResMat <- lapply(womenFiles, extractVariables)

# verify we have 14 separate entries for each year 1999 - 2012
length(womenResMat)
```

```{r}
# verify row count again
sapply(womenResMat, nrow)
```

#### Build of ExtractVariables function for assembling consistent data

```{r}
# formatting ages as numeric
age <- map(womenResMat, ~ as.numeric(.x[ ,'ag']))
```

```{r}
# we some have missing values
sapply(age, function(x) sum(is.na(x)))
```

```{r,  results='hide'}
# some of these are due to comments
# can update the extractVariables function

extractVariables = 
  function(file, varNames =c("name", "home", "ag", "gun", "net", "time"))
  {
    
    # Find the index of the row with =s
    eqIndex = grep("^===", file)
    # Extract the two key rows and the data 
    spacerRow = file[eqIndex] 
    headerRow = tolower(file[ eqIndex - 1 ])
    body = file[ -(1 : eqIndex) ]
    # Remove footnotes and blank rows
    footnotes = grep("^[[:blank:]]*(\\*|\\#)", body)
    if ( length(footnotes) > 0 ) body = body[ -footnotes ]
    blanks = grep("^[[:blank:]]*$", body)
    if (length(blanks) > 0 ) body = body[ -blanks ]
    
    
    # Obtain the starting and ending positions of variables   
    searchLocs = findColLocs(spacerRow)
    locCols = selectCols(varNames, headerRow, searchLocs)
    
    Values = mapply(substr, list(body), start = locCols[1, ], 
                    stop = locCols[2, ])
    colnames(Values) = varNames
    
    return(Values)
  }

# update matrix
womenResMat = lapply(womenFiles, extractVariables)

```

```{r}
# recheck missing values
# results are better
sapply(age, function(x) sum(is.na(x)))
```

#### Identification of ages under seven

```{r}
# we have some potential bad data
# 2001 has a racer under age 7
sapply(age, function(x) which(x < 7))
```

#### Race time reformatting

```{r,  results='hide'}
# NOTE: race times are not all in the same format
charTime = womenResMat[['2012']][, 'time']
head(charTime)
tail(charTime)

# Converting time by str_split, then mapping them to numeric.
convertTime <- function(t) {
  timePieces <- str_split(t, ":")
  map_dbl(timePieces, function(x) {
    x <- as.numeric(x)
    if (length(x) == 2) 
      x[1] + x[2]/60 
    else 60 * x[1] + x[2] + x[3]/60
  })
}


# Creating dataframe
createDf = function(Res, year, sex) {
  if (!is.na(Res[1, "net"])) 
    useTime = Res[, "net"] else if (!is.na(Res[1, "gun"])) 
      useTime = Res[, "gun"] else useTime = Res[, "time"]
      
      useTime = gsub("[#\\*[:blank:]]", "", useTime)
      runTime = convertTime(useTime[useTime != ""])
      
      Res = Res[useTime != "", ]
      age = gsub("X{2}\\s{1}?|\\s{3}?", "0  ", Res[, "ag"])
      Res[, "ag"] = age
      
      Results = data.frame(year = rep(year, nrow(Res)), 
                           sex = rep(sex, nrow(Res)), 
                           name = Res[, "name"], 
                           home = Res[, "home"], 
                           age = as.numeric(Res[, "ag"]), 
                           runTime = runTime, 
                           stringsAsFactors = FALSE)
      invisible(Results)
}

```

### 2006 Spacer Anomoly

#### Root Cause: the extractVariables() function would aggregate hometown and time columns due to the ==== spacer row not aligning properly. Fixing this issue allowed the data to import properly, else all 5432 lines will import incorrectly

```{r,  results='hide'}
# gets every line that starts with ===
separatorIdx <- grep("^===", womenFiles[["2006"]])

# filters the list to 2006
separatorRow <- womenFiles[["2006"]][separatorIdx]

# makes a separator row
paste(substring(separatorRow, 1, 63), " ", substring(separatorRow, 65, nchar(separatorRow)), sep = "") -> separatorRowX

# replaces the === with the separator row
womenFiles[["2006"]][separatorIdx] <- separatorRowX

# extracts vars from the files
womenResMat <- sapply(womenFiles, extractVariables)

# makes a list of data frames from these things
womenDF <- mapply(createDf, womenResMat, year = 1999:2012, sex = rep("W", 14), SIMPLIFY = FALSE)

# investigage: HUGE DF w/ year hierarchy
summary(womenDF)
str(womenDF)

# collapse into DF w/ year separated out from 1999 - 2012
allWomen <- do.call(rbind, womenDF)

# sorting by year, then run times
# allWomen <- allWomen %>% dplyr::arrange(year, runTime)
allWomen[order(allWomen$year, allWomen$runTime),] -> allWomen
```

### Runners with age of zero
#### We finally get to a workable and cleaned dataset to use for analysis

```{r}
# we see 23 runners with an age of 0
count(allWomen[allWomen$age == 0, ])

# removing rows with 0 / NA's in age column
allWomen <- allWomen[allWomen$age != 0, ]

# summary stats for womens age
allWomen %>% group_by(year) %>% summarise(ag_mean = mean(age, na.rm = T), 
                                          ag_max = max(age, na.rm = T), 
                                          ag_min = min(age, na.rm = T), 
                                          ag_median = median(age, na.rm = T), 
                                          ag_sd = sd(age, na.rm = T)) 
#double check format of workable data frame
head(allWomen[allWomen$year == '2006',])
```

## Race Analysis

#### Overall distribution of woman's ages from 1999-2012 using a box and whisker plot. The box and whisker plot is a useful way to visualize differences among samples or groups. The results of this plot point to fairly consistent dispersion in data. The median does not show drastic differences, although one could argue the race has gotten slight younger due to the lack of women over the age of 77 from 2008-2012. The more narrow distributions at the end of the whiskers during these years indicate this.

```{r box/whisker plot}
# Look at womens box plots: filter out ages younger than 7
age %>% enframe(name = "year", value = "age") %>% unnest() %>% filter(age, age > 7) %>% ggplot(aes(year, age)) + geom_boxplot() + ggtitle("Women's Ages 1999-2012")
```

#### Age distribution for all years shows a right skewed distribution, with a mean age of 33.8. No clear evidence of a shift in runner age from 1999 to 2012.

```{r}
# age distribution for all years
allWomen %>% ggplot() + geom_density(aes(x = age), alpha = 0.7,color="darkblue", fill="lightblue") + ggtitle("Cherry Blossom Women - Age all years") + geom_vline(aes(xintercept=mean(age)),
            color="blue", linetype="dashed", size=1)
mean(allWomen$age)
```

```{r}
list_of_values = c('1999','2012')
allWomen %>% filter(age > 7 & year == list_of_values) %>% ggplot() + geom_density(aes(x = age,group = factor(year), fill=factor(year), alpha = 0.7)) + ggtitle("Cherry Blossom Women - Age (1999 & 2012)") + labs(fill = "Year")
```

#### Run time distribution for all years shows a much more symmetric distribution.

```{r}
# womens run time for all years
allWomen %>% ggplot() + geom_density(aes(x = runTime), alpha = 0.7,color="darkblue", fill="lightblue") + ggtitle("Cherry Blossom Women - Run Time") + geom_vline(aes(xintercept=mean(runTime)),
            color="blue", linetype="dashed", size=1)
```

#### The charts below highlight the following race insights: Race attendence has steadily increased over the years 1999-2012, average woman's run time has increased, and average run time appears to increase as the ages get higher. 

```{r}
# womens total attendance for all years
allWomen %>% group_by(year) %>% summarise(Attendance = n()) %>% na.omit %>% ggplot() + geom_line(aes( x = year, y = Attendance)) + ggtitle("Cherry Blossom Women Total Attendance 1999-2012")
```

```{r}
# womens average run time for all years
allWomen %>% group_by(year) %>% summarise(`Average runTime` = mean(runTime, na.rm = T)) %>% ggplot() + geom_line(aes(x = year, y = `Average runTime`)) + ggtitle("Cherry Blossom Women Annual Run Time")
```

```{r}
# womens average run time by age
allWomen %>% group_by(age) %>% summarise(`Average runTime` = mean(runTime, na.rm = T)) %>% ggplot(aes(x = age, y = `Average runTime`)) + geom_point() + geom_smooth() + ggtitle("Cherry Blossom Women Run Time By Age - LOESS Smoothing")
```

#### We see some erratic behavior at the lower and higher ends of the age spectrum. Let's examine the situation and search for changepoints in the data since we don't know the exact age where this variation in average run time has occured. We'll also use a bootstrapping method and include confidence intervals.

## Change-point Analysis

### Single Changepoint

```{r}
set.seed(1)
pw.model <- piecewise.linear(allWomen$age, allWomen$runTime, middle = 1, CI=TRUE, bootstrap.samples = 100, sig.level = 0.05)
pw.model
```

#### Note a possible changepoint (threshold alpha) at age 44. Our model coefficients are also shown, as well as a confidence interval of our change point (43.4 - 45.7). Important to note here: "0" is not included in any of our confidence intervals, which is evidence to reject the null hypothesis, meaning there appears to be evidence of there being a changepoint in average run time at age 44.

### Testing for Multiple Changepoints. 

#### In reality, there may be multiple changes in this race dataset. To that end, we utilize the Pruned Exact Linear Time (PELT) method to search the solution space in the most efficient manner. We use the CROPS penalty in conjunction with the PELT method in this situation. This method utilizes a non-parametric cost function based on the empirical distribution of the data. The diagnostic plot gives us an idea of how many changes to choose (the point on the elbow). Finally, a new plot shows the location of the changepoints (ages 38 and 66).

```{r}
df = allWomen %>% group_by(age) %>% summarise(`Average runTime` = mean(runTime, na.rm = T))
df = data.frame(
  x = df$age,
  y = df$`Average runTime`)

fit_changepoint <- cpt.np(df$y, penalty = "CROPS", pen.value = c(25,100), method="PELT",
                       test.stat="empirical_distribution",class=TRUE,minseglen=2, nquantiles =4*log(length(df$x)))

plot(fit_changepoint, diagnostic = TRUE)
plot(fit_changepoint, ncpts = 2)
fit_changepoint
```
