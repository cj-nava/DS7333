---
title: "Unit 10 - Imputation with Boston Housing Data"
author: "Martin Garcia, Michael Catalano, Jeremy Otsap, Christian Nava"
date: "7/8/2020"
output: 
  html_document:
    keep_md: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(VIM)
library(skimr)
library(na.tools)
library(mice) #used for multiple imputation methods
library(reshape2) # used to change our data shape from long to short and vice versa
library(corrplot) # generates a correlation matrix for us
library(naniar) # used to generate visualizations for missing data
```

With the Boston housing data as our backdrop we observe the effect of different imputation methods ranging from single imputation via a global mean to multiple imputation methods. However this is dependent on missing data types, including missing completely at random, missing at random, and not missing at random. We assume the  data 'missing completely at random' is a result of the flooding in the basement where the housing records are stored. We also highlight the effect of listwise deletion on two extreme examples, data missing 1% and 50% for our feature PTRATIO. Data missing at random removes data for our columns CRIM and RAD that are dependent on Age. With intent we remove TAX data for our example of 'not missing at random' by removing data TAX data when it is equal to or less than 279. 

## Boston Data Set 

The Boston housing data set contains median housing prices collected bt the US Census Services and contains quantitative feature data such as the crime rate by town or the average number of rooms per dwelling.


## Initial Data Exploration

**Data Description**

The Boston data frame has 506 rows and 14 columns.

This data frame contains the following columns:
  
* CRIM - per capita crime rate by town
* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
* INDUS - proportion of non-retail business acres per town.
* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
* NOX - nitric oxides concentration (parts per 10 million)
* RM - average number of rooms per dwelling
* AGE - proportion of owner-occupied units built prior to 1940
* DIS - weighted distances to five Boston employment centres
* RAD - index of accessibility to radial highways
* TAX - full-value property-tax rate per $10,000
* PTRATIO - pupil-teacher ratio by town
* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* LSTAT - % lower status of the population
* MEDV - Median value of owner-occupied homes in $1000's

```{r, echo=FALSE}

# loading data set

boston.df <- read.csv("https://raw.githubusercontent.com/jotsap/DS7333/master/data/boston.csv", header = T)

```



**Data Exploration**

The dataset reflects numeric values both integer and float type data. 

```{r, echo=FALSE}

#explore data frame

str(boston.df)
summary(boston.df)


```


Skim allows a quick visual examination of each variable's range

```{r, echo=FALSE}

#skim dataframe
skim(boston.df)


```



**Data Visualization**


```{r, echo=FALSE}

ggplot(data = melt(boston.df), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable)) + coord_flip()
```


Initially created a correlation matrix to view the impact of multicollinearity for predictor variables that show high correlation.

```{r}

boston_cor<-cor(boston.df)
corrplot(boston_cor, method="color")
```

**Missing Data**

No missing values found in our initial Boston data set as validated below.

```{r, echo=FALSE}

# from VIM package
aggr(boston.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```



# Initial Regression Model

We are doing a regression on full data set, predicting MEDV, the median value of owner-occupied homes, using the other 13 parameters.


**Exclude vs Omit**

Using na.exclude pads the residuals and fitted values with NAs where there were missing values. Other functions do not use the na.action, but instead have a different argument (with some default) for how they will handle missing values. For example, the mean command will, by default, return NA if there are any NAs in the passed object. The example below creates a threshold to measure the effect of different imputations.


```{r, echo=FALSE}

# initial full regression

boston.lm <- lm( MEDV ~ . , data = boston.df, na.action = na.omit  )

# model results for goodniess of fit using adj r-sqr
summary(boston.lm)

# sse for loss
print("SSE for model is: ")
sum(boston.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston.lm$residuals ^ 2)


```


**RESULTS**

Improvements for different imputation methods can be measured with various statistics. Adjusted R-square or coefficient of determination can help us assess the models overall quality. Adjusted R-squared value would help us explain the proportion of variance of median house prices with the given predictor variables while also penalizing for large number of predictors. Fitting a linear regression model without imputation results in a R-square of .7338, which means roughly 73.4% of the MEDV response variable is accounted for by the other 13 predictors. The accuracy of the model can be captured by looking at our residuals or actual median house prices minus predicted house prices. The baseline model results in a total sum of square errors of 11078. The goal will be to have a large R-square and small sum of squares error value. These results reflect the effect of listwise deletion on data missing completely at random (MCAR).

<center>
The Sum of Squares Error [SSE] for model is: <b>11078.78</b>

The Mean Square Error [MSE] for model is: <b>21.89483</b>

Adjusted R-squared: <b>.7338</b>
</center>

NOTE: Because AGE and INDUS were found to not be statistically significant we will *not* use those for the missing data exercises



# Missing Completely at Random with Regression: Single Imputation & Multiple Imputation
<br>

We will measure the effect of single imputation and multiple imputation using the PTRATIO, or pupil to teacher ratio. For our single imputation method we will use a global mean to replace our missing values for PTRATIO. This is under the backdrop of observations that are missing completely at random, or MCAR. We assume the housing records were flooded and we lost a random number of data sets at different rates. We will also use listwise deletion for our data sets missing 1% and 50% to measure the magnified effect to our mean square error and adjusted R-square.

First generate vector of *unique* random numbers to correspond to each percentage

* 1% - 5 entries
* 5% - 25 entries
* 10% - 51 entries
* 20% - 102 entries
* 33% - 168 entries
* 50% - 253 entries 


**Value to be Deleted**
<br>
PTRATIO - pupil-teacher ratio by town

Create vector to serve as sample index for creating NA values

```{r, echo=FALSE}

# replace set to FALSE ensures no repeating numbers in sample vector

# 1% - 5 entries
base::sample(1:506, 5, replace = F) -> index_01

# 5% - 25 entires
base::sample(1:506, 25, replace = F) -> index_05

# 10% - 50 entires
base::sample(1:506, 51, replace = F) -> index_10

# 20% - 100 entries
base::sample(1:506, 102, replace = F) -> index_20

# 33% - 165 entries
base::sample(1:506, 168, replace = F) -> index_33

# 50% - 250 entries 
base::sample(1:506, 253, replace = F) -> index_50

```



**Create different boston samples with NA for PTRATIO**

We visualize the prorportion of missing value sets ranging from 1% to 50%, with their corresponding volume.

1% - 5 entries 

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_01.df
# assign NA to PTRATIO
boston_mcar_01.df[index_01,'PTRATIO'] <- NA
# validate results

head(boston_mcar_01.df[index_01,'PTRATIO'])

aggr(boston_mcar_01.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```



5% - 25 entries

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_05.df
# assign NA to PTRATIO
boston_mcar_05.df[index_05,'PTRATIO'] <- NA
# validate results

head(boston_mcar_05.df[index_05,'PTRATIO'])

aggr(boston_mcar_05.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```


10% - 51 entries

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_10.df
# assign NA to PTRATIO
boston_mcar_10.df[index_10,'PTRATIO'] <- NA
# validate results

head(boston_mcar_10.df[index_10,'PTRATIO'])

aggr(boston_mcar_10.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```


20% - 102 entries

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_20.df
# assign NA to PTRATIO
boston_mcar_20.df[index_20,'PTRATIO'] <- NA
# validate results

head(boston_mcar_20.df[index_20,'PTRATIO'])

aggr(boston_mcar_20.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```


33% - 168 entries

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_33.df
# assign NA to PTRATIO
boston_mcar_33.df[index_33,'PTRATIO'] <- NA
# validate results

head(boston_mcar_33.df[index_33,'PTRATIO'])

aggr(boston_mcar_33.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```


50% - 253 entries

```{r, echo=FALSE}

# create separate file
boston.df -> boston_mcar_50.df
# assign NA to PTRATIO
boston_mcar_50.df[index_50,'PTRATIO'] <- NA
# validate results

head(boston_mcar_50.df[index_50,'PTRATIO'])

aggr(boston_mcar_50.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```



## Single Imputation with Mean



**1% - 5 entries**

Adjusted R-squared:  0.7344
[1] "SSE for model is: "
[1] 11052.98
[1] "MSE for model is: "
[1] 21.84383


```{r, echo=FALSE}

# mean substitution
boston_mcar_01.df$PTRATIO <- na.mean(boston_mcar_01.df$PTRATIO)

# regression model
boston_mcar_01.lm <- lm( MEDV ~ . , data = boston_mcar_01.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_01.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_01.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_01.lm$residuals ^ 2)


```



**5% - 25 entries**

Adjusted R-squared:  0.7313

[1] "SSE for model is: "
[1] 11182.17
[1] "MSE for model is: "
[1] 22.09915

```{r, echo=FALSE}

# mean substitution
boston_mcar_05.df$PTRATIO <- na.mean(boston_mcar_05.df$PTRATIO)

# regression model
boston_mcar_05.lm <- lm( MEDV ~ . , data = boston_mcar_05.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_05.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_05.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_05.lm$residuals ^ 2)


```



**10% - 51 entries**

Adjusted R-squared:  0.7345 
[1] "SSE for model is: "
[1] 11049.58
[1] "MSE for model is: "
[1] 21.83712

```{r, echo=FALSE}

# mean substitution
boston_mcar_10.df$PTRATIO <- na.mean(boston_mcar_10.df$PTRATIO)

# regression model
boston_mcar_10.lm <- lm( MEDV ~ . , data = boston_mcar_10.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_10.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_10.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_10.lm$residuals ^ 2)


```



**20% - 102 entries**

Adjusted R-squared:  0.7231 
[1] "SSE for model is: "
[1] 11523.58
[1] "MSE for model is: "
[1] 22.77388

```{r, echo=FALSE}

# mean substitution
boston_mcar_20.df$PTRATIO <- na.mean(boston_mcar_20.df$PTRATIO)

# regression model
boston_mcar_20.lm <- lm( MEDV ~ . , data = boston_mcar_20.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_20.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_20.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_20.lm$residuals ^ 2) 


```



**33% - 168 entries**

Adjusted R-squared:  0.7205 
[1] "SSE for model is: "
[1] 11632.02
[1] "MSE for model is: "
[1] 22.98819

```{r, echo=FALSE}

# mean substitution
boston_mcar_33.df$PTRATIO <- na.mean(boston_mcar_33.df$PTRATIO)

# regression model
boston_mcar_33.lm <- lm( MEDV ~ . , data = boston_mcar_33.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_33.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_33.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_33.lm$residuals ^ 2) 


```



**50% - 253 entries** 

Adjusted R-squared:  0.7142 
[1] "SSE for model is: "
[1] 11894.22
[1] "MSE for model is: "
[1] 23.50637

```{r, echo=FALSE}

# mean substitution
boston_mcar_50.df$PTRATIO <- na.mean(boston_mcar_50.df$PTRATIO)

# regression model
boston_mcar_50.lm <- lm( MEDV ~ . , data = boston_mcar_50.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mcar_50.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mcar_50.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mcar_50.lm$residuals ^ 2) 


```

Overall our adjusted R-square value fell as we increased the amount of missing data and filled with a global mean. In contrast our sum of square increased as we filled more missing data with a global mean. 

## Listwise Deletion at 1% MCAR and 50% MCAR

The listewise approach will remove the 1% and 50% missing data and model thereafter using our lm function.

```{r}
lwd1 <- lm( MEDV ~ . , data = boston_mcar_01.df, na.action = na.omit  )

# model results for goodness of fit using adj r-sqr
summary(lwd1)

```

```{r, echo=FALSE}

# sse for loss
print(paste0("SSE for model is:  ", sum(lwd1$residuals ^ 2)))

# mse for loss
print(paste0("MSE for model is: ", mean(lwd1$residuals ^ 2)))

```

```{r}
lwd50 <- lm( MEDV ~ . , data = boston_mcar_50.df, na.action = na.omit  )

# model results for goodness of fit using adj r-sqr
summary(lwd50)
```

```{r, echo=FALSE}
# sse for loss
print(paste0("SSE for model is: ",sum(lwd50$residuals ^ 2)))


# mse for loss
print(paste0("MSE for model is: ",mean(lwd50$residuals ^ 2)))


```


## Multiple Imputation with Mice

We impute the data with Mice with default parameters and return 5 different imputations that are then pooled together. The default imputation method is applied or predictive mean matching. This method pools samples of complete data set, or uses actual values for those with complete data for our student to teacher ratio.

```{r,results = 'hide'}  
imp_01 <- mice(boston_mcar_01.df,  m = 5)
imp_05 <- mice(boston_mcar_05.df,  m = 5)
imp_10 <- mice(boston_mcar_10.df,  m = 5)
imp_20 <- mice(boston_mcar_20.df,  m = 5)
imp_33 <- mice(boston_mcar_33.df,  m = 5)
```

The function mice below will create 5 different imputation sets. The imputation sets are identified via the iter column.

```{r}
imp_50 <- mice(boston_mcar_50.df,  m = 5)
```

```{r}

head(imp_50$imp$PTRATIO)

```

We incorporate all 5 imputation possibilities back into our data and transform it into a long dataframe. Each of these imputed values represent a sample from a distribution. 

```{r, echo=FALSE}
boston.lm01_long <- complete(imp_01, action="long", include = TRUE)
boston.lm05_long <- complete(imp_05, action="long", include = TRUE)
boston.lm10_long <- complete(imp_10, action="long", include = TRUE)
boston.lm20_long <- complete(imp_20, action="long", include = TRUE)
boston.lm33_long <- complete(imp_33, action="long", include = TRUE)
```

```{r}
boston.lm50_long <- complete(imp_50, action="long", include = TRUE)

```

We can see there are 5 new datasets with their respective imputed data sets, 0 acting as our original data set.

```{r}

table(boston.lm50_long$.imp)

```

We change our dataframe to type a mids or multiply imputed object.

```{r, echo=FALSE}

# Convert back to mids type - mice can work with this type
imp_long_mids01 <- as.mids(boston.lm01_long)
imp_long_mids05 <- as.mids(boston.lm05_long)
imp_long_mids10 <- as.mids(boston.lm10_long)
imp_long_mids20 <- as.mids(boston.lm20_long)
imp_long_mids33 <- as.mids(boston.lm33_long)
```

```{r}
imp_long_mids50 <- as.mids(boston.lm50_long)
```

We run our linear model using this object which runs all 5 different iterations of our data set. Using the pool function from Mice we take the average of every model. 

```{r, echo=FALSE}

fitimp01 <- with(imp_long_mids01,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp05 <- with(imp_long_mids05,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp10 <- with(imp_long_mids10,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp20 <- with(imp_long_mids20,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp33 <- with(imp_long_mids33,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
```


```{r}
fitimp50 <- with(imp_long_mids50,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))

```

Multiple imputation results with 50% missing data.

```{r}
summary(pool(fitimp50))

pool.r.squared(fitimp50, adjusted=TRUE)

```

Results for other missing data sets.

```{r}
pool.r.squared(fitimp01, adjusted=TRUE)
pool.r.squared(fitimp05, adjusted=TRUE)
pool.r.squared(fitimp10, adjusted=TRUE)
pool.r.squared(fitimp20, adjusted=TRUE)
pool.r.squared(fitimp33, adjusted=TRUE)
```

Interestingly our adjusted r-squared value remain pretty constant when filling out missing data from a distribution pool from our multiple imputation method. 

## Results

Notable was imputation using single imputation and listwise deletion when the amount of missing data was small. The effect of removing those observations resulted in a no differences to our adjusted R-square value when compared against a single imputation approach at an adjusted R-square of 73.11. This is close to as high as we can go with this linear model as the adjusted R-square for our original model with all the data is at 73.38. If we are ok with these results all approaches above work well, however if we want to optimize our model we might want to change to something completely different as the linear model creates a ceiling we can't exceed with imputation alone. We can however improve the model optimizing the model itself as imputation does little.

# Missing at Random with Regression: Single Imputation & Multiple Imputation

**Determining Parameter for Deletion**
<br>
AGE < 80 = 266 entries 

**Values to be Deleted**
<br>
CRIM - per capita crime rate by town
<br>
RAD - index of accessibility to radial highways

```{r, results = 'hide'}

# list rows with AGE < 80 
boston.df[boston.df$AGE < 80,c("CRIM","RAD")]


# create initial MAR index
rownames( boston.df[boston.df$AGE < 80,c("CRIM","RAD")] ) %>% as.numeric() -> index_mar

#create 10% MAR index
sample(index_mar, 50 ) -> index_mar_10

#create 20% MAR index
sample(index_mar, 100) -> index_mar_20

#create 30% MAR index
sample(index_mar, 150) -> index_mar_30


```


**Replacement**

```{r}

# create 10% MAR dataframe
boston.df -> boston_mar_10.df

# create 20% MAR dataframe
boston.df -> boston_mar_20.df

# create 30% MAR dataframe
boston.df -> boston_mar_30.df


#assign NA to CRIM & RAD for 10% MAR
boston_mar_10.df[index_mar_10, c("CRIM","RAD")] <- NA

#assign NA to CRIM & RAD for 20% MAR
boston_mar_20.df[index_mar_20, c("CRIM","RAD")] <- NA

#assign NA to CRIM & RAD for 30% MAR
boston_mar_30.df[index_mar_30, c("CRIM","RAD")] <- NA


```

**Validation **

We validate that 10, 20, and 30 percent of our data is missing with a quick glance at the graphs below.

```{r}
gg_miss_var(boston_mar_10.df, show_pct = TRUE)
gg_miss_var(boston_mar_20.df, show_pct = TRUE)
gg_miss_var(boston_mar_30.df, show_pct = TRUE)
```


### MAR 10%

When AGE < 80, 10% of the entries for CRIM and RAD will be replaced with the NA value


**Validation**

```{r, echo=FALSE, results = 'hide'}

### 10% MAR

# list rows with missing values
boston_mar_10.df[is.na(boston_mar_10.df$CRIM),c("CRIM","RAD","AGE")]

# list rows with AGE < 80 
boston_mar_10.df[boston_mar_10.df$AGE < 80, c("CRIM","RAD","AGE")]

```




## Single Imputation with Mean

**Regression Using Mean Substitution**

Adjusted R-squared:  0.7312 
[1] "SSE for model is: "
[1] 11188.48
[1] "MSE for model is: "
[1] 22.11163

```{r, echo=FALSE}

# mean substitution for CRIM
boston_mar_10.df$CRIM <- na.mean(boston_mar_10.df$CRIM)

#mean substitution for RAD
boston_mar_10.df$RAD <- na.mean(boston_mar_10.df$RAD)

# regression model
boston_mar_10.lm <- lm( MEDV ~ . , data = boston_mar_10.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mar_10.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mar_10.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mar_10.lm$residuals ^ 2) 



```


### MAR 20%

When AGE < 80, 20% of the entries for CRIM and RAD will be replaced with the NA value


**Validation**

```{r, echo=FALSE,results = 'hide' }

### 20% MAR

# list rows with missing values
boston_mar_20.df[is.na(boston_mar_20.df$CRIM),c("CRIM","RAD","AGE")]

# list rows with AGE < 80 
boston_mar_20.df[boston_mar_20.df$AGE < 80, c("CRIM","RAD","AGE")]

```


**Regression Using Mean Substitution**

Adjusted R-squared:  0.7292 
[1] "SSE for model is: "
[1] 11267.98
[1] "MSE for model is: "
[1] 22.26873

```{r, echo=FALSE}

# mean substitution for CRIM
boston_mar_20.df$CRIM <- na.mean(boston_mar_20.df$CRIM)

#mean substitution for RAD
boston_mar_20.df$RAD <- na.mean(boston_mar_20.df$RAD)

# regression model
boston_mar_20.lm <- lm( MEDV ~ . , data = boston_mar_20.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mar_20.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mar_20.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mar_20.lm$residuals ^ 2) 



```


### MAR 30%

When AGE < 80, 30% of the entries for CRIM and RAD will be replaced with the NA value


**Validation**

```{r, echo=FALSE, results = 'hide'}

### 30% MAR

# list rows with missing values
boston_mar_30.df[is.na(boston_mar_30.df$CRIM),c("CRIM","RAD","AGE")]

# list rows with AGE < 80 
boston_mar_30.df[boston_mar_30.df$AGE < 80, c("CRIM","RAD","AGE")]

```


**Regression Using Mean Substitution**

Adjusted R-squared:  0.7325 
[1] "SSE for model is: "
[1] 11133.25
[1] "MSE for model is: "
[1] 22.00247

```{r, echo=FALSE}

# mean substitution for CRIM
boston_mar_30.df$CRIM <- na.mean(boston_mar_30.df$CRIM)

#mean substitution for RAD
boston_mar_30.df$RAD <- na.mean(boston_mar_30.df$RAD)

# regression model
boston_mar_30.lm <- lm( MEDV ~ . , data = boston_mar_30.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mar_30.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mar_30.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mar_30.lm$residuals ^ 2) 



```

## Multiple Imputation with Mice

```{r}

imp_10 <- mice(boston_mar_10.df,  m = 5)
imp_20 <- mice(boston_mar_20.df,  m = 5)
imp_30 <- mice(boston_mar_30.df,  m = 5)

```

```{r}

boston.lm10_long <- complete(imp_10, action="long", include = TRUE)
boston.lm20_long <- complete(imp_20, action="long", include = TRUE)
boston.lm30_long <- complete(imp_30, action="long", include = TRUE)

```

```{r}
imp_long_mids10 <- as.mids(boston.lm10_long)
imp_long_mids20 <- as.mids(boston.lm20_long)
imp_long_mids30 <- as.mids(boston.lm30_long)
```

```{r}
fitimp10 <- with(imp_long_mids10,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp20 <- with(imp_long_mids20,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
fitimp30 <- with(imp_long_mids30,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
```

Results for 10% missing data modeled after multiple imputation applied.

```{r}
summary(pool(fitimp10))

pool.r.squared(fitimp10, adjusted=TRUE)
```

Results for 20% missing data modeled after multiple imputation applied.

```{r}
summary(pool(fitimp20))

pool.r.squared(fitimp20, adjusted=TRUE)
```

Results for 30% missing data modeled after multiple imputation applied.

```{r}
summary(pool(fitimp30))

pool.r.squared(fitimp30, adjusted=TRUE)
```

## Results

There were no significant differences with our adjusted r-square values between imputation methods, either single or multiple. However that doesnt mean the imputation didn't do its job well as it closely matched our adjusted r-squared value of 73.38 for our full data set modeled with a linear regression model.

# Missing Not at Random with Regression: Single Imputation & Multiple Imputation

We explicitly select what data is missing which creates data imputation for data that is missing not at random.

When TAX <= 279 entries will be replaced with the NA value

* TAX <= 279 [128 values 25.3%]


```{r, echo=FALSE, results = 'hide'}

# create 10% MAR dataframe
boston.df -> boston_mnar_25.df

# list rows with TAX <= 279 
boston_mnar_25.df[boston_mnar_25.df$TAX <= 279,]


```


```{r, echo=FALSE}

#assign NA to TAX <= 279 for 25% MNAR
#boston_mnar_25.df[boston_mnar_25.df$TAX <= 279, ] <- NA

boston_mnar_25.df$TAX <- ifelse(boston_mnar_25.df$TAX <= 279, NA, boston_mnar_25.df$TAX) 


```

**Validation**

We validate the missing values which should account for 25% of the variable, TAX. The resulting NA's volume at 25% is 128.

```{r, echo=FALSE}

aggr(boston_mnar_25.df, 
     prop = FALSE, 
     combined = TRUE, 
     numbers = TRUE, 
     sortVars = TRUE, 
     sortCombs = TRUE)

```

## Single Imputation with Mean

**Regression Using Mean Substitution**

Adjusted R-squared:  0.7292 
[1] "SSE for model is: "
[1] 11269.16
[1] "MSE for model is: "
[1] 22.27106

```{r}

# mean substitution for TAX
boston_mnar_25.df$TAX <- na.mean(boston_mnar_25.df$TAX)

# regression model
boston_mnar_25.lm <- lm( MEDV ~ . , data = boston_mnar_25.df )

# model results for goodniess of fit using adj r-sqr
summary(boston_mnar_25.lm)

# sse for loss
print("SSE for model is: ")
sum(boston_mnar_25.lm$residuals ^ 2)

# mse for loss
print("MSE for model is: ")
mean(boston_mnar_25.lm$residuals ^ 2) 



```

## Multiple Imputation with Mice

```{r}

imp_25 <- mice(boston_mnar_25.df,  m = 5)
boston.lm25_long <- complete(imp_25, action="long", include = TRUE)
imp_long_mids25 <- as.mids(boston.lm25_long)
fitimp25 <- with(imp_long_mids25,
               lm(MEDV ~   CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT))
summary(pool(fitimp25))

pool.r.squared(fitimp25, adjusted=TRUE)


```

## Results

There are not major differences between are adjusted R-square value that remains at .7292 for both a single and multiple imputation method. Our takeaway was the resulting adjusted r-squared or residuals did not show major differences for either single or multiple imputation methods. However what we though initially thought of a a negative result was really positive given the context that being the model we are using, the adjusted r-square and residuals closely matched those of our full data set. Meaning the imputation methods were able to mime the original data set and resulted in similar metrics. Using Listwise deletion resulted in adjusted r-squared and residuals similar to our full data model when the amount of missing data was small. Imputing when we had 50% missing data using Listwise deletion gave use our smalled adjusted r-squared value and sum of square results at .7121 and 11979 respectively.


# References
<br>
**https://www.kaggle.com/puxama/bostoncsv** 
<br>
https://www.kaggle.com/c/boston-housing
<br>
https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html